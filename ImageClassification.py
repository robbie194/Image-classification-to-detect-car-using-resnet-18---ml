# -*- coding: utf-8 -*-
"""Assignment2_B00792835.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JbWWAdu0NqqZSdOjFi49Fk6qB0sv3yR6
"""

#@title CSCI 6515 Machine Learning for Big Data { form-width: "100%" }

#@title Assignment 2 / Project { form-width: "100%" }

Name    = 'SRISAICHAND SINGAMANENI' #@param {type:"string"}
Program = 'Masters' #@param ["PhD", "Masters", "Undergraduate"]
ID      = 'B00792835' #@param {type:"string"}

from os import path
from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag
from google.colab import files
import zipfile

platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())

accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'

!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision
  
!pip install Pillow==4.0.0
!pip install PIL
!pip install image

print("Please select the Images Zip File named as 'images.zip'")
uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

from zipfile import ZipFile
zip = ZipFile("images.zip")
zip.extractall()

print("Please select the CSV File named as 'train_cars.csv'")

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

import pandas as pd
data = pd.read_csv('train_cars.csv', sep=',',header=0)
data['id'] = data.groupby(['target']).ngroup()

print(data)

classes = data.groupby("target").size()
print("Total Classes available in the data:")
print((classes))

print("Number of classes:")
num_classes = len(classes)
# print(num_classes)

from __future__ import print_function
from __future__ import division
import torch
import torch.nn as nn
import torch.optim as optim
import tensorflow as tf
import numpy as np
import torchvision
from torchvision import datasets, models, transforms
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import PIL
from PIL import Image
import time
import image
import os
import copy
print("PyTorch Version: ",torch.__version__)
print("Torchvision Version: ",torchvision.__version__)

trainvalid, test = train_test_split(data, test_size=0.1, random_state=10, stratify=data[['target']])
train, valid = train_test_split(trainvalid, test_size=0.1, random_state=10)

def train_model(model, dataloaders, criterion, optimizer, num_epochs):
    since = time.time()

    val_acc_history = []

    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0

    for epoch in range(num_epochs):
        print('Epoch {}/{}'.format(epoch, num_epochs - 1))
        print('-' * 10)

        # Each epoch has a training and validation phase
        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()  # Set model to training mode
            else:
                model.eval()   # Set model to evaluate mode

            running_loss = 0.0
            running_corrects = 0

            # Iterate over data.
            for inputs, labels in dataloaders[phase]:
                inputs = inputs.to(device)
                labels = labels.to(device)

                # zero the parameter gradients
                optimizer.zero_grad()

                # forward
                # track history if only in train
                with torch.set_grad_enabled(phase == 'train'):
                    # Get model outputs and calculate loss
                    
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)

                    _, preds = torch.max(outputs, 1)

                    # backward + optimize only if in training phase
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                # statistics
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / len(dataloaders[phase].dataset)
            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)

            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))

            # deep copy the model
            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())
            if phase == 'val':
                val_acc_history.append(epoch_acc)

        print()

    time_elapsed = time.time() - since
    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))
    print('Best val Acc: {:4f}'.format(best_acc))

    # load best model weights
    model.load_state_dict(best_model_wts)
    return model, val_acc_history
  
  #Core reference: https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html

def initialize_model(num_classes, use_pretrained):
  model_ft = models.resnet18(pretrained=use_pretrained)
  num_ftrs = model_ft.fc.in_features
  model_ft.fc = nn.Linear(num_ftrs, num_classes)
  return model_ft

model_ft = initialize_model(num_classes, use_pretrained=True)
print(model_ft)

# Helper Class CarsDataSet to load images from the folder and assign labels

class CarsDataset(Dataset):
    def __init__(self, labels, root_dir, subset=False, transform=None):
        self.labels = labels
        self.root_dir = root_dir
        self.transform = transform
    
    def __len__(self):
        return len(self.labels)
    
    def __getitem__(self, idx):
        img_name = self.labels.image_name.iloc[idx]
        fullname = os.path.join(self.root_dir, img_name)
        image = Image.open(fullname)
        
        labels = self.labels.id.iloc[idx]
        
        if self.transform:
            image = self.transform(image)
        return [image, labels]

# Helper class to load and return Images

class PhotosData(Dataset):
    def __init__(self, labels, root_dir, subset=False, transform=None):
        self.labels = labels
        self.root_dir = root_dir
        self.transform = transform
    
    def __len__(self):
        return len(self.labels)
    
    def __getitem__(self, idx):
        img_name = self.labels.image_name.iloc[idx]
        fullname = os.path.join(self.root_dir, img_name)
        image = Image.open(fullname)
        labels = self.labels.id.iloc[idx]
        if self.transform:
            image = self.transform(image)
        return image

# Code to compute Mean and Standard Deviation of the Images

# Since it is taking a lot of time to compute, the calculated values are directly assigned.

# from torchvision import datasets, models, transforms
# photos = PhotosData(labels=train, root_dir='images/',
#         transform=transforms.Compose([transforms.Resize([224,224]),
#         transforms.ToTensor()]))

# import itertools
# import statistics

# R_Channel=[]
# G_Channel=[]
# B_Channel=[]

# for photo in photos:
#   rch = np.reshape(photo[:,:,0], -1)
#   R_Channel.append(rch.tolist())
#   gch =  np.reshape(photo[:,:,1], -1)
#   G_Channel.append(gch.tolist())
#   bch =  np.reshape(photo[:,:,2], -1)
#   B_Channel.append(bch.tolist())


# R_Channel = list(itertools.chain(*R_Channel))
# G_Channel = list(itertools.chain(*G_Channel))
# B_Channel = list(itertools.chain(*B_Channel))

# Rch_mean = np.mean(R_Channel)
# Gch_mean = np.mean(G_Channel)
# Bch_mean = np.mean(B_Channel)

# Rch_std = np.std(R_Channel)
# Gch_std = np.std(G_Channel)
# Bch_std = np.std(B_Channel)

# print("******Mean*******")
# print(Rch_mean, Gch_mean, Bch_mean)
# print("*****Standard Deviation*******")
# print(Rch_std, Gch_std, Bch_std)

# Assigned Mean and Standard deviation values for R , G , B , Channels.

Rchan_mean = 0.5687456641176981 
Gchan_mean = 0.5687456641176981 
Bchan_mean = 0.4844112531742014

Rchan_std = 0.24747782814116578
Gchan_std = 0.24747782814116578
Bchan_std = 0.24536295503695874

#Transforming the data, preprocessing, and Taking it to dataloaders.

normalize = transforms.Normalize(mean = (Rchan_mean, Gchan_mean, Bchan_mean), std = (Rchan_std, Gchan_std, Bchan_std))
train_trans = transforms.Compose([transforms.Resize([224, 224]),
                               transforms.RandomHorizontalFlip(),
                               transforms.ToTensor(),normalize])
valid_trans = transforms.Compose([transforms.Resize([224, 224]),
                               transforms.ToTensor(),normalize])
test_trans = transforms.Compose([transforms.Resize([224, 224]),
                               transforms.ToTensor(),normalize])

train_ds = CarsDataset(labels=train, root_dir='images/', transform=train_trans)
valid_ds = CarsDataset(labels=valid, root_dir='images/', transform=valid_trans)
test_ds = CarsDataset(labels=test, root_dir='images/', transform=test_trans)


train_dl = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=4)
valid_dl = DataLoader(valid_ds, batch_size=64, shuffle=True, num_workers=4)
test_dl = DataLoader(test_ds, batch_size=64, shuffle=True, num_workers=4)


dataloader_dict = {'train':train_dl, 'val':valid_dl}
dataloader_test = {'test':test_dl}

print(dataloader_dict)
print(dataloader_test)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Send the model to GPU
model_ft = model_ft.to(device)

# Gather the parameters to be optimized/updated in this run. If we are
#  finetuning we will be updating all parameters. However, if we are
#  doing feature extract method, we will only update the parameters
#  that we have just initialized, i.e. the parameters with requires_grad
#  is True.
feature_extract = True
params_to_update = model_ft.parameters()
print("Params to learn:")
if feature_extract:
    params_to_update = []
    for name,param in model_ft.named_parameters():
        if param.requires_grad == True:
            params_to_update.append(param)
            print("\t",name)
else:
    for name,param in model_ft.named_parameters():
        if param.requires_grad == True:
            print("\t",name)

# Observe that all parameters are being optimized
optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)

#Code reference: https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html

# Setup the loss fxn
criterion = nn.CrossEntropyLoss()

# Train and evaluate
model_prelearned, hist = train_model(model_ft, dataloader_dict, criterion, optimizer_ft, num_epochs=10)

#Function for Testing the model

def test_model(model_ft, dataloader):
    model_ft.eval()
    test_acc = 0.0
    running_corrects = 0
    for inputs, labels in dataloader['test']:

#         if cuda_avail:
        inputs = inputs.to(device)
        labels = labels.to(device)
    
        # Predict classes using images from the test set
        outputs = model_ft(inputs)
        _, prediction = torch.max(outputs.data, 1)
        running_corrects += torch.sum(prediction == labels.data)

    test_acc = running_corrects.double() / len(dataloader['test'].dataset)
#         test_acc += torch.sum(prediction == labels.data)

  
    return test_acc

#Testing the model accuracy with test data

test_prelearn = test_model(model_prelearned, dataloader_test)

# Saving the model with best validation Accuracy

torch.save(model_prelearned.state_dict(),'TrainedModel_Pretrained.pth')

# savedins = initialize_model(15, use_pretrained=True)
# saved = savedins.load_state_dict(torch.load('TrainedModel.pth'))
# saved_model = (torch.load('TrainedModel'))
# saved_model = model_ft.load_state_dict(saved_model)

print(test_prelearn)

# Initialize the non-pretrained version of the model used for this run

scratch_model = initialize_model(num_classes, use_pretrained=False)
scratch_model = scratch_model.to(device)
scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.001, momentum=0.9)
scratch_criterion = nn.CrossEntropyLoss()
non_pre_model,scratch_hist = train_model(scratch_model, dataloader_dict, scratch_criterion, scratch_optimizer, num_epochs=10)

# Testing and saving the model for the one trained from scratch
test_acc_nonpre = test_model(non_pre_model, dataloader_test)

torch.save(non_pre_model.state_dict(),'TrainedModel_Scratch.pt')

# savedmodel = initialize_model(num_classes, use_pretrained=False)
# savedmodel = savedmodel.to(device)
# savedmodel = savedmodel.load_state_dict(torch.load('TrainedModel_Scratch.pt'))

print(test_acc_nonpre)

# Plotting the accuracies

import matplotlib.pyplot as plt

ohist = []
shist = []

ohist = [h.cpu().numpy() for h in hist]
shist = [h.cpu().numpy() for h in scratch_hist]


plt.subplot(2,1,1)

plt.title("Validation Accuracy vs. Number of Training Epochs")
plt.xlabel("Training Epochs")
plt.ylabel("Validation Accuracy")

plt.plot(range(1,10+1),ohist,label="Pretrained")
plt.plot(range(1,10+1),shist,label="Scratch")
plt.legend()
plt.ylim((0,1.))
plt.xticks(np.arange(1, 10+1, 1.0))

plt.subplot(2,1,2)

plt.title("Test Accuracy of Pretrained and scratch models")
plt.xlabel("")
plt.ylabel("Test Accuracy")

plt.hlines(test_acc_nonpre, 0, 2, color='r', label="Test_Scratch")
plt.hlines(test_prelearn, 0, 2, color='b', label="Test_Pretrained")
plt.ylim((0,1.))
plt.xticks(np.arange(1, 1, 1.0))
plt.legend()
plt.show()

#End of File

#References:

# 1 https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html
# 2 https://www.kaggle.com/pvlima/use-pretrained-pytorch-models
# 3 https://pytorch.org/docs/stable/data.html
# 4 https://pytorch.org/docs/stable/torchvision/transforms.html